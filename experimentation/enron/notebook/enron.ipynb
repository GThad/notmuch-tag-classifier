{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enron Classifier Test\n",
    "\n",
    "This notebook gives the Python code for our initial tests of building a classifier with scikit-learn and the labelled Enron email database from Berkeley.\n",
    "\n",
    "First we need to get to the folder where the emails are stores and get the list of emails (.txt) and their corresponding category files (.cats) and create tuples out of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD is C:\\Users\\mthadath\\Desktop\\gifan\\notmuch-tag-classifier\\experimentation\\enron\\notebook\n",
      "Email Directory is C:/Users/mthadath/Desktop/gifan/notmuch-tag-classifier/experimentation/enron/emails\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import re\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(\"CWD is {}\".format(cwd))\n",
    "cwdList = cwd.split(\"\\\\\")\n",
    "enronWd = \"/\".join(cwdList[0:-1] + [\"emails\"])\n",
    "os.chdir(enronWd)\n",
    "print(\"Email Directory is {}\".format(enronWd))\n",
    "\n",
    "fileList = os.listdir()\n",
    "emailFileList = [file for file in fileList\n",
    "                 if fnmatch.fnmatch(file, \"*.txt\")]\n",
    "catFileList = [file for file in fileList\n",
    "               if fnmatch.fnmatch(file, \"*.cats\")]\n",
    "\n",
    "emailFileList.sort()\n",
    "catFileList.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check for pairwise matching of email and cat IDs\n",
    "for email, cat in zip(emailFileList, catFileList):\n",
    "    emailID = email.split(\".\")[0]\n",
    "    catID = cat.split(\".\")[0]\n",
    "    if(emailID != catID):\n",
    "        print(\"ERROR: emailID {} differs from catID {}\".\n",
    "               format(emailID, catID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now that we have our email file and corresponding category file strings in tuples, we can begin reading the data in. For now each we will read each email in as a 4 tuple (w,x,y,z) where w is the ID, x is the email headers, y is the email message, and z is a list of tags associated with it. Since we use a slightly modified tagging system than the database, we need to translate their tags to our tags. First we initalize the renaming dictionary and then create the list of 4 tuples.\n",
    "\n",
    "Note they use the word \"cat\" for labels, we use the word \"tag\". This is convenient to distinguish their labels from ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note their categories are in the form \"x,y,z\". We use z = 2\n",
    "# to ensure both labellers agreed.\n",
    "catToTagID = {\"1,1,2\": 1,   \"1,3,2\": 2,   \"3,1,2\": 3,   \"3,2,2\": 4,\n",
    "              \"3,3,2\": 5,   \"3,3,2\": 6,   \"3,4,2\": 5,   \"3,4,2\": 7,\n",
    "              \"3,5,2\": 8,   \"3,6,2\": 9,   \"3,7,2\": 10,  \"3,7,2\": 11,\n",
    "              \"3,8,2\": 10,  \"3,8,2\": 12,  \"3,9,2\": 13,  \"3,10,2\": 14,\n",
    "              \"3,11,2\": 15, \"3,12,2\": 16, \"3,13,2\": 17, \"1,2,2\": 18,\n",
    "              \"1,4,2\": 19,  \"1,5,2\": 20,  \"1,6,2\": 21,  \"1,7,2\": 22,\n",
    "              \"1,8,2\": 22,  \"2,4,2\": 23,  \"2,5,2\": 24,  \"2,5,2\": 25,\n",
    "              \"2,6,2\": 24,  \"2,6,2\": 26,  \"2,7,2\": 27,  \"2,10,2\": 28,\n",
    "              \"2,11,2\": 29, \"2,12,2\": 29}\n",
    "tagIDToTagName = {1: \"company\", 2: \"company-personal\",\n",
    "                  3: \"company-regulations\", 4: \"company-strategy\",\n",
    "                  5: \"company-image\", 6: \"company-image-current\",\n",
    "                  7: \"company-image-future\", 8: \"company-contributers\",\n",
    "                  9: \"company-california-crisis\", 10: \"company-internal\",\n",
    "                  11: \"company-internal-policy\", 12: \"company-internal-operations\",\n",
    "                  13: \"company-allies\", 14: \"company-legal\",\n",
    "                  15: \"company-talk-points\", 16: \"company-minutes\",\n",
    "                  17: \"company-trip-reports\", 18: \"personal\",\n",
    "                  19: \"logistics\", 20: \"career\", 21: \"collaboration\", \n",
    "                  22: \"empty\", 22: \"empty\", 23: \"news-article\", 24: \"gov\",\n",
    "                  25: \"gov-report\", 26: \"gov-action\", 27: \"press-release\",\n",
    "                  28: \"newsletter\", 29: \"joke\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "emailTuples = list()\n",
    "\n",
    "for emailFile, catFile in zip(emailFileList, catFileList):\n",
    "    emailID = int(emailFile.split(\".\")[0])\n",
    "    with open(emailFile, \"r\") as email:\n",
    "        emailLines = email.readlines()\n",
    "        headerSepIndex = emailLines.index(\"\\n\")\n",
    "        headers = \"\".join(emailLines[:headerSepIndex])\n",
    "        msg = \"\".join(emailLines[headerSepIndex + 1:])\n",
    "    with open(catFile, \"r\") as cats:\n",
    "        tags = list()\n",
    "        for cat in cats:\n",
    "            cat = cat.rstrip(\"\\n\")\n",
    "            if cat in catToTagID:\n",
    "                tags.append(catToTagID[cat])\n",
    "        tags.sort()\n",
    "                \n",
    "    emailTuples.append((emailID, headers, msg, tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the emails in a list of tuples, we have to remove the ones without any tags in them.\n",
    "\n",
    "Possible issue: apparently running the code below multiple times only gets rid of all the tagless emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1181\n"
     ]
    }
   ],
   "source": [
    "for email in emailTuples:\n",
    "    if not email[3]:\n",
    "        emailTuples.remove(email)\n",
    "        \n",
    "print(len(emailTuples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output, it seems we went from our original 1702 emails to 1181 for our dataset.\n",
    "\n",
    "Now we need to convert each email message into a set of features. This is the feature generation step.\n",
    "\n",
    "Remember, we can generate features from the headers and from the actual message. The problem is that we generate features of the headers in a different way from the message. We could somehow create a giant matrix for the data, but this would involve implementing all the feature generation code ourselves. Luckily scikit-learn comes with many routines for feature generation.\n",
    "\n",
    "In particular, they have tools to generate word feature vectors from the email messages, such as their TfidfVectorizer. They do not have tools for generating features from email headers, so we will need to implement those. Then there is the question of how to combine the different feature matrices. Thankfully, they also have a feature union class that does exactly this behind the scenes. We can also use their pipelining system to create a pipeline of operations where the output of one operation is the input of the other. We will set up an entire pipeline to generate features, perform feature selection, and then train the classifier. The benefit of using a pipeline will be the ability to swap in or add different routines for each step.\n",
    "\n",
    "Before we get into implementing the pipeline, we need to figure out some issues that could arise.\n",
    "\n",
    "Looking at the source code, the TdidfVectorizer only removes accents and makes terms lowercase as a preprocessing step. We need to these both and remove certain words from the message which do not appear in natural text. These include numbers or odd strings like \"xyz=abc\" or links or file paths of some sort -- really anything that isn't prose. Essentially, we want to remove non alphabet characters from each message, but we need to be careful about this. Whenever a non-alphabet character separates two actual words, removing the character can create an unnatural string. For example removing non-alphabet characters from \"www.hello.com/world\" would create \"wwwhellocomworld\". This isn't useful. A better heuristic to use would be to split any word on its non-alphabet characters (creating [\"www\", \"hello\", \"com\", \"world\"]), and then add the resulting words to the word bag.\n",
    "\n",
    "The implementation of our preprocessor is given below. We will be able to send it as an argument to the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Expects a string msg\n",
    "def cleanMsg(msg):\n",
    "    words = msg.split(\" \")\n",
    "    words[:] = [re.split(\"[^a-z|A-Z]\", word) for word in words]\n",
    "    words[:] = [word for sublist in words for word in sublist]\n",
    "    words[:] = [word for word in words if word is not \"\"]\n",
    "    words[:] = [word.lower() for word in words]\n",
    "    return(\" \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"To\" header can take a comma separated list of email addresses as a value. Scikit-learn only handles numeric data, so we could use a one-hot encoding, but the one-hot encoder assumes that each feature value is a single value. In this case, we have a feature value being a list. Instead, we can use the MultiLabelBinarizer to handle lists, which generalized one-hot encoding. This should give us a feature matrix we need for each header.\n",
    "\n",
    "The pipeline is implemented below. Note that it is actually pipelines within pipelines. We base it off of [this](http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html#sphx-glr-auto-examples-hetero-feature-union-py) tutorial from the scikit-learn documentation. Currently it does not implement all the steps we want it to, but it is a good start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# A wrapper class over selecting a value from a dictionary. We\n",
    "# wrap this functionality to make it usable within the scikit-learn\n",
    "# pipeline interface. If previous step in the pipeline\n",
    "# outputs a collection implementing getitem, i.e. we can get a value\n",
    "# using data[key], then we can get the actual value.\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return(self)\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return(data_dict[self.key])\n",
    "\n",
    "# A transformer expecting the email 4-tuple. It will extract the header\n",
    "# string and the message string from each email. In the end this creates\n",
    "# a dictionary with keys \"header\" and \"message\" that will return\n",
    "# the list of header or message strings.\n",
    "class HeaderMsgExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return(self)\n",
    "\n",
    "    def transform(self, emailTuples):\n",
    "        return({\"header\": [email[1] for email in emailTuples],\n",
    "                \"message\": [email[2] for email in emailTuples]})\n",
    "    \n",
    "# A transformer expecting to operate on a list of header strings.\n",
    "# When initialized, it takes a string of the actual header to\n",
    "# extract from the header string.\n",
    "class HeaderExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, headerToExtract):\n",
    "        self.headerToExtract = headerToExtract\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return(self)\n",
    "    \n",
    "    def getHeaderValue(self, header):\n",
    "        header = header.split(\"\\n\")\n",
    "        headerWithKey = [line for line in header \n",
    "                         if re.match(self.headerToExtract, line)]\n",
    "        # Quick and dirty way to handle case when header is missing\n",
    "        if not headerWithKey:\n",
    "            return(\"none\")\n",
    "        value = headerWithKey[0].split(\":\")[1].strip()\n",
    "        return(value)\n",
    "    \n",
    "    def transform(self, headerStrings):\n",
    "        convertedData = list()\n",
    "        for headerString in headerStrings:\n",
    "            headerValue = self.getHeaderValue(headerString).split(\",\")\n",
    "            headerValue[:] = [value.strip() for value in headerValue]\n",
    "            headerValue[:] = [value for value in headerValue if value is not \"\"]\n",
    "            convertedData.append(headerValue)\n",
    "        return(convertedData)\n",
    "\n",
    "# Simple debugging transformer that will print its input and then return it.\n",
    "class PrintTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return(self)\n",
    "    \n",
    "    def transform(self, input):\n",
    "        print(input)\n",
    "        return(input)\n",
    "\n",
    "# A simple wrapper over the MultiLabelBinarizer to make it work with\n",
    "# the pipeline system. More specifically it needs to avoid implementing\n",
    "# fit_transform since the pipeline will call that and send 3 arguments.\n",
    "# The fit_transform function for a MultiLabelBinarizer accepts only 2 arguments.\n",
    "class MultiLabelBinarizerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return(self)\n",
    "    \n",
    "    def transform(self, input):\n",
    "        return(MultiLabelBinarizer().fit_transform(input))\n",
    "    \n",
    "pipeline = Pipeline([\n",
    "    # Extract the header and message\n",
    "    ('headermsg', HeaderMsgExtractor()),\n",
    "    # Use FeatureUnion to combine feature matrices generated from \n",
    "    # header and msg\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list =\n",
    "        [\n",
    "            # Pipeline for generating features from the \"To\" header\n",
    "            ('gen_to_feats', Pipeline([('selector', ItemSelector(key='header')),\n",
    "                                  ('header_extractor', HeaderExtractor(\"To\")),\n",
    "                                          ('encode', MultiLabelBinarizerTransformer())])\n",
    "            ),\n",
    "            # Pipeline for generating features from the \"From\" header\n",
    "            ('gen_from_feats', Pipeline([('selector', ItemSelector(key='header')),\n",
    "                                  ('header_extractor', HeaderExtractor(\"From\")),\n",
    "                                          ('encode', MultiLabelBinarizerTransformer())])\n",
    "            ),\n",
    "            # Pipeline for generate bag-of-words from message\n",
    "            ('gen_msg_feats', Pipeline([('selector', ItemSelector(key='message')),\n",
    "                                   ('tfidf', TfidfVectorizer(preprocessor=cleanMsg))])\n",
    "            )\n",
    "        ],\n",
    "    )),\n",
    "    ('print')\n",
    "    # Use a SVC classifier on the combined features\n",
    "    ('svc', LinearSVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now that we have the pipeline built, we are half way ready for training it! We still have to generate the target values. Now note there are multiple tags assigned to each email. For now, we will keep things simple and focus on building teh classifier to classify for a single tag, namely \"company\". Fortunately, scikit-learn implements tools to deal with classification of a feature that can take a list of values. We will touch into that later. First we need to see if our pipeline works at all.\n",
    "\n",
    "To begin, we should use the MultiLabelBinarizer, to get the tag data encoded in a useful way. Since we are only classifying one feature right now we will just take part of the resulting feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "emailTargets = MultiLabelBinarizer(classes = range(1,30)).fit_transform([email[3] \n",
    "                                                                         for email in emailTuples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright now that we have a feature matrix of our target values, let us get the first column which corresponds to the \"company\" tag. Then lets put it all through the pipeline and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 19825 features per sample; expecting 20087",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-301-03ee59bd44f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memailTuples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m600\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memailTargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m600\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memailTuples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m600\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m700\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memailTargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m600\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m700\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mscore_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample_weight'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mscore_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \"\"\"\n\u001b[0;32m    348\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \"\"\"\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 305\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 19825 features per sample; expecting 20087"
     ]
    }
   ],
   "source": [
    "clf = pipeline.fit(emailTuples[:600], emailTargets[:600,0])\n",
    "prediction = clf.score(emailTuples[600:700], emailTargets[600:700,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens is an error! We get a ValueError with message \"X has 19825 features per sample; expecting 20087.\" It may not be clear what the problem is here. Essentially running the training set through the pipeline, ended up getting us 20087 features while running the testing set through the pipeline, ended up getting us 19825 features. Note that we never really told the MultiLabelBinarizer what our entire set of tags was, so if the training set and testing set had different sets of tags, then the discrepency translates to the feature matrix. The same sort of thing happened since we didn't send TdidfVectorizer a set of all possible words. In the end, our implementation has not taken into account that the test data could be missing some features and also introduce new features. So the task is to implement some way of making the training and testing data have the same features. Note, however, we can't simply send a total list of features since we will never be able to know what it is!\n",
    "\n",
    "During the training phase, we will need to save the end list of features. Since each feature is really some column index, we will want to generate string feature names for each index. During the testing phase, we will generate an initial feature matrix from the testing data, but then we will prune the feature matrix depending on our saved list of features. Missing features will be added on, and given a \"missing value\" marker of some sort. We can then use scikit-learn's tools for dealing with missing values. Unseen features will be removed entirely. If we don't remove them, then we would somehow have to add the feature to the training feature matrix, which would be awkward to accomplish. The question then is, how do we modify the current pipeline to support this.\n",
    "\n",
    "The FeatureUnion class allows us to get the feature names of the combined feature matrix. However, it does this by concantenating the results from getting the feature names of the individual transformations in the list of transformations. Since our transformations are actually a pipeline of transformations, which don't implement get_feature_names, this concatenated list of features can't be made. We should reduce these pipelines of operations into a single transformation, so we can implement get_feature_names in them. We prefer this since it would remove the pipelines inside of pipelines, which may help with implementing parameter search later.\n",
    "\n",
    "Now that we know how to get the features, we need to figure out how to save them. It is tempting to do this in a transformer, but it will not work since no step in a pipeline has access to previous steps. This means we won't be able to get the feature names. Thus we will have to subclass Pipeline since it does have access to all the steps in the pipeline, namely the feature union. In our subclass, we will overwrite the fit and predict methods so that the featrues of the training data are saved, and the features of the testing data are filled/pruned as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Wrapper over Pipeline to implement handling of missing/unseen features\n",
    "# in test data\n",
    "class EmailPipeline(Pipeline):\n",
    "    def __init__(self, steps, memory=None):\n",
    "        super().__init__(steps, memory)\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        fitclf = super().fit(X, y, **fit_params)\n",
    "        self.trainFeatureNames = self.getFeatureNames()\n",
    "        return(fitclf)\n",
    "        \n",
    "    # Implementation is exactly the same as the scikit-learn implementation\n",
    "    # with a step to modify test data results\n",
    "    def predict(self, X):\n",
    "        Xt = X\n",
    "        for name, transform in self.steps[:-1]:\n",
    "            if transform is not None:\n",
    "                Xt = transform.transform(Xt)\n",
    "        Xt = self.fixTestFeatures(Xt)\n",
    "        return self.steps[-1][-1].predict(Xt)\n",
    "    \n",
    "    def getFeatureNames(self):\n",
    "        for name, transform in self.steps[:-1]:\n",
    "            if type(transform) is FeatureUnion:\n",
    "                return(transform.get_feature_names())\n",
    "        \n",
    "    def fixTestFeatures(self, X):\n",
    "        testFeatureNames = self.getFeatureNames()\n",
    "        testIndicesToKeep = [index for index, featureName in enumerate(testFeatureNames) \n",
    "                             if featureName in self.trainFeatureNames]\n",
    "        testFeatureNames = [featureName for index, featureName in enumerate(testFeatureNames) \n",
    "                            if featureName in self.trainFeatureNames]\n",
    "        Xt = X[:, testIndicesToKeep]\n",
    "        \n",
    "        missingTestFeatureIndices = [index for index, featureName in enumerate(self.trainFeatureNames) \n",
    "                                     if featureName not in testFeatureNames]\n",
    "\n",
    "        for index in missingTestFeatureIndices:\n",
    "            left = Xt[:,:index]\n",
    "            newCol = np.zeros(Xt.shape[0])[:, None]\n",
    "            right = Xt[:,index:]\n",
    "            Xt = hstack([left, newCol, right]).tocsr()\n",
    "        \n",
    "        return(Xt)\n",
    "    \n",
    "# Combines functions of ItemSelector, HeaderExtractor, and MultiLabelBinarizer\n",
    "class HeaderExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, headerToExtract):\n",
    "        self.headerToExtract = headerToExtract\n",
    "        self.binarizer = MultiLabelBinarizer()\n",
    "    \n",
    "    def _getHeaderValue(self, header):\n",
    "        header = header.split(\"\\n\")\n",
    "        headerWithKey = [line for line in header \n",
    "                         if re.match(self.headerToExtract, line)]\n",
    "        # Quick and dirty way to handle case when header is missing\n",
    "        if not headerWithKey:\n",
    "            return(\"none\")\n",
    "        value = headerWithKey[0].split(\":\")[1].strip()\n",
    "        return(value)\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return(self)\n",
    "    \n",
    "    def transform(self, emailInfo):\n",
    "        headerStrings = emailInfo[\"header\"]\n",
    "\n",
    "        convertedData = list()\n",
    "        for headerString in headerStrings:\n",
    "            headerValue = self._getHeaderValue(headerString).split(\",\")\n",
    "            headerValue[:] = [value.strip() for value in headerValue]\n",
    "            headerValue[:] = [value for value in headerValue if value is not \"\"]\n",
    "            convertedData.append(headerValue)\n",
    "        return(self.binarizer.fit_transform(convertedData))\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return(list(self.binarizer.classes_))\n",
    "\n",
    "# Combines functions of ItemSelector and TfidfVectorizer\n",
    "class BagOfWords(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, **vectorizerArgs):\n",
    "        self.vectorizer = TfidfVectorizer(vectorizerArgs)\n",
    "    \n",
    "    def transform(self, emailInfo):\n",
    "        return(self.vectorizer.fit_transform(emailInfo[\"message\"]))\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return(self)\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return(self.vectorizer.get_feature_names())\n",
    "        \n",
    "pipeline = EmailPipeline([\n",
    "    # Extract the header and message\n",
    "    ('headermsg', HeaderMsgExtractor()),\n",
    "    # Use FeatureUnion to combine feature matrices generated from \n",
    "    # header and msg\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list =\n",
    "        [\n",
    "            # Pipeline for generating features from the \"To\" header\n",
    "            ('to', HeaderExtractor(\"To\")),\n",
    "            # Pipeline for generating features from the \"From\" header\n",
    "            ('from', HeaderExtractor(\"From\")),\n",
    "            # Pipeline for generate bag-of-words from message\n",
    "            ('word', BagOfWords(preprocessor=cleanMsg))\n",
    "        ]\n",
    "    )),\n",
    "    # Use a SVC classifier on the combined features\n",
    "    ('svc', LinearSVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "shuffledEmailTuples = shuffle(emailTuples, random_state = 1)\n",
    "shuffledEmailTargets = MultiLabelBinarizer(classes = range(1,30)).fit_transform([email[3] \n",
    "                                                                         for email in shuffledEmailTuples])\n",
    "\n",
    "clf = pipeline.fit(shuffledEmailTuples[:1000], shuffledEmailTargets[:1000,0])\n",
    "prediction = pipeline.predict(shuffledEmailTuples[1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0\n",
      " 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0\n",
      " 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0\n",
      " 1 0 0 1 1 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 1 0 1 0 1\n",
      " 1 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0]\n",
      "[0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0\n",
      " 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0\n",
      " 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 0\n",
      " 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1\n",
      " 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.76795580110497241"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(prediction)\n",
    "print(shuffledEmailTargets[1000:,0])\n",
    "np.mean(prediction == shuffledEmailTargets[1000:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
